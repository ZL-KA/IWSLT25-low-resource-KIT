trainer:
  group_by_length: True
  per_device_train_batch_size: 16
  per_device_eval_batch_size: 1
  gradient_accumulation_steps: 1
  evaluation_strategy: 'steps'
  num_train_epochs: 10 
  fp16: True
  gradient_checkpointing: True
  save_steps: 1000
  eval_steps: 500
  logging_steps: 100
  learning_rate: 1e-5
  weight_decay: 0.005
  warmup_steps: 500
  save_total_limit: 3
  load_best_model_at_end: True
  metric_for_best_model: 'loss'
  greater_is_better: False
  seed: 42
  data_seed: SEED
  label_smoothing_factor: 0.0
  # lr_scheduler_type: 'inverse_sqrt' #https://github.com/huggingface/transformers/blob/v4.38.2/src/transformers/optimization.py#L296
  lr_scheduler_type: 'linear'
  early_stopping_patience: 20
preprocessing:
  do_data_aug: False